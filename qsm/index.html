<!DOCTYPE html> <html lang="en"> <head> <meta name="google-site-verification" content="XyWrmS0s13oZm3YGw5ruPZ8_SFL_MASFAkyTMdk-DkM"/> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Learning a Diffusion Model Policy from Rewards via Q-Score Matching | Alejandro Escontrela</title> <meta name="author" content="Alejandro Escontrela"/> <meta name="description" content="Learning diffusion model policies for off-policy reinforcement learning."/> <meta name="keywords" content="AI, robotics, ML, artificial, intelligence, reinforcement, learning"/> <meta property="og:site_name" content="Alejandro Escontrela"/> <meta property="og:type" content="website"/> <meta property="og:title" content="Alejandro Escontrela | Learning a Diffusion Model Policy from Rewards via Q-Score Matching"/> <meta property="og:url" content="https://Alescontrela.github.io/qsm/"/> <meta property="og:description" content="Learning diffusion model policies for off-policy reinforcement learning."/> <meta property="og:locale" content="en"/> <meta name="twitter:card" content="summary"/> <meta name="twitter:title" content="Learning a Diffusion Model Policy from Rewards via Q-Score Matching"/> <meta name="twitter:description" content="Learning diffusion model policies for off-policy reinforcement learning."/> <meta name="twitter:site" content="@alescontrela"/> <meta name="twitter:creator" content="@alescontrela"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <script src="https://cdn.jsdelivr.net/npm/hls.js@1"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/pdf.js/3.11.174/pdf.min.js"></script> <link rel="stylesheet" href="https://unpkg.com/flickity@2/dist/flickity.min.css"> <script src="https://unpkg.com/flickity@2/dist/flickity.pkgd.min.js"></script> <script src="https://unpkg.com/flickity-fade@1/flickity-fade.js"></script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.5.1/css/all.min.css" integrity="sha256-wiz7ZSCn/btzhjKDQBms9Hx4sSeUYsDrTLg7roPstac=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/uc_logo.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://alescontrela.github.io/qsm/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class=" sticky-bottom-footer"> <header> <nav id="navbar" class="navbar d-block d-sm-none navbar-light navbar-expand sticky-top"> <div class="container justify-content-start"> <a class="navbar-brand nav-link justify-content-start" href="/" style="margin-right: 0;"> <img class="z-depth-1 rounded" style="display: flex; margin: auto;" src="https://imagedelivery.net/3j6TRSJdJ6zKbW2C_AD0hA/a4596165-6f70-4b50-79af-51d30197de00/public" height="80em" width="80em" max-width="100%" data-zoomable> </a> <div> <span><h4 style="margin-top: 0.55em; margin-bottom: 0.25em;"> <span class="font-weight-bold">Alejandro</span> Escontrela</h4></span> <span><p>AI Researcher</p></span> </div> </div> <div class="container d-flex justify-content-around"> <ul class="navbar-nav flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/photography/">photo</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="toggle-container"> <button class="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </nav> <nav id="navbar" class="navbar d-none d-sm-block navbar-light navbar-expand sticky-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="https://Alescontrela.github.io/">Alejandro Escontrela</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/photography/">photo</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog</a> </li> <li class="toggle-container"> <button class="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <article> <div class="row"> <div class="text-center col-6 col-sm-6 mt-6 mt-md-0"> <h3><a href="https://arxiv.org/abs/2312.11752" target="_blank" rel="noopener noreferrer">Paper<br><i class="fas fa-file-pdf"></i></a></h3> </div> <div class="text-center col-6 col-sm-6 mt-6 mt-md-0"> <h3><a href="https://github.com/Alescontrela/score_matching_rl" target="_blank" rel="noopener noreferrer">Code<br><i class="fas fa-file-code"></i></a></h3> </div> </div> <p><br></p> <h2 class="no_toc" id="publication-info">Publication info</h2> <hr> <div class="publications"> <ol class="bibliography"><li> <div> <div class="row no-gutters"> <div class="col-sm-4 mx-auto my-auto"> <img src="/assets/img/score_matching_rl/qsm_splash.png" class="card-img" style="max-width: 100%;"> </div> <div class="col-sm-8"> <div class="card-body"> <div id="Psenka24arXiv_QSM"> <div class="title"> <a href="https://arxiv.org/abs/2312.11752" target="_blank" rel="noopener noreferrer">Learning a Diffusion Model Policy from Rewards via Q-Score Matching</a> </div> <div class="author"> <a href="https://www.michaelpsenka.io/" target="_blank" rel="noopener noreferrer">Michael Psenka</a>*,  <em>Alejandro Escontrela</em>*, <a href="https://people.eecs.berkeley.edu/~pabbeel/" target="_blank" rel="noopener noreferrer">Pieter Abbeel</a>, and <a href="https://scholar.google.com/citations?user=XqLiBQMAAAAJ&amp;hl=en" target="_blank" rel="noopener noreferrer">Yi Ma</a> </div> <div class="periodical"> <em>International Conference on Maching Learning</em> 2024 </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2312.11752" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/Alescontrela/score_matching_rl" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="/qsm" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Psenka24arXiv_QSM</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Maching Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Psenka, Michael and Escontrela, Alejandro and Abbeel, Pieter and Ma, Yi}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Artificial Intelligence (cs.AI)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning a Diffusion Model Policy from Rewards via Q-Score Matching}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{Creative Commons Attribution 4.0 International}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2312.11752}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </div> </div> </div> </li></ol> </div> <p><br></p> <h2 id="abstract">Abstract</h2> <hr> <p>Diffusion models have become a popular choice for representing actor policies in behavior cloning and offline reinforcement learning. This is due to their natural ability to optimize an expressive class of distributions over a continuous space. However, previous works fail to exploit the score-based structure of diffusion models, and instead utilize a simple behavior cloning term to train the actor, limiting their ability in the actor-critic setting. In this paper, we present a theoretical framework linking the structure of diffusion model policies to a learned Q-function, by linking the structure between the score of the policy to the action gradient of the Q-function. We focus on off-policy reinforcement learning and propose a new policy update method from this theory, which we denote <em>Q score matching</em>. Notably, this algorithm only needs to differentiate through the denoising model rather than the entire diffusion model evaluation, and converged policies through Q-score matching are implicitly multi-modal and explorative in continuous domains. We conduct experiments in simulated environments to demonstrate the viability of our proposed method and compare to popular baselines.</p> <p><br></p> <h2 class="no_toc" id="acknowledgements">Acknowledgements</h2> <hr> <p>Michael Psenka acknowledges support from ONR grant N00014-22-1-2102. Alejandro Escontrela acknowledges support from an NSF Fellowship, NSF NRI #2024675. Yi Ma acknowledges support from ONR grant N00014-22-1-2102 and the joint Simons Foundation-NSF DMS grant #2031899. This work was partially supported by NSF 1704458, the Northrop Grumman Mission Systems Research in Applications for Learning Machines (REALM) initiative, NIH NIA 1R01AG067396, and ARO MURI W911NF-17-1-0304.</p> <p><br></p> <h2 class="no_toc" id="how-to-cite">How to cite</h2> <hr> <div class="publications"> <ol class="bibliography"><li> <div> <div class="row no-gutters"> <div class="col-sm-12"> <div class="card-body"> <div id="Psenka24arXiv_QSM"> <div class="bibtex"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">Psenka24arXiv_QSM</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{International Conference on Maching Learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Psenka, Michael and Escontrela, Alejandro and Abbeel, Pieter and Ma, Yi}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{Artificial Intelligence (cs.AI)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Learning a Diffusion Model Policy from Rewards via Q-Score Matching}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">copyright</span> <span class="p">=</span> <span class="s">{Creative Commons Attribution 4.0 International}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
  <span class="na">eprint</span> <span class="p">=</span> <span class="s">{2312.11752}</span><span class="p">,</span>
  <span class="na">archiveprefix</span> <span class="p">=</span> <span class="s">{arXiv}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.LG}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </div> </div> </div> </li></ol> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Alejandro Escontrela. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/copy_bib.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://cdnjs.cloudflare.com/ajax/libs/js-polyfills/0.1.43/polyfill.min.js"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-0LS8VY9HJQ"></script> <script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-0LS8VY9HJQ");</script> </body> </html>